{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a77a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c10f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8684c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class RMSLE(object):\n",
    "    def calc_ders_range(self, approxes, targets, weights):\n",
    "        assert len(approxes) == len(targets)\n",
    "        if weights is not None:\n",
    "            assert len(weights) == len(approxes)\n",
    "\n",
    "        result = []\n",
    "        for index in range(len(targets)):\n",
    "            val = max(approxes[index], 0)\n",
    "            der1 = math.log1p(targets[index]) - math.log1p(max(0, approxes[index]))\n",
    "            der2 = -1 / (max(0, approxes[index]) + 1)\n",
    "\n",
    "            if weights is not None:\n",
    "                der1 *= weights[index]\n",
    "                der2 *= weights[index]\n",
    "\n",
    "            result.append((der1, der2))\n",
    "        return result\n",
    "class RMSLE_val(object):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return np.sqrt(error / (weight + 1e-38))\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        return False\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        assert len(approxes) == 1\n",
    "        assert len(target) == len(approxes[0])\n",
    "\n",
    "        approx = approxes[0]\n",
    "\n",
    "        error_sum = 0.0\n",
    "        weight_sum = 0.0\n",
    "\n",
    "        for i in range(len(approx)):\n",
    "            w = 1.0 if weight is None else weight[i]\n",
    "            weight_sum += w\n",
    "            error_sum += w * ((math.log1p(max(0, approx[i])) - math.log1p(max(0, target[i])))**2)\n",
    "\n",
    "        return error_sum, weight_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b25b88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CAT_COLS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lp/v_4h0mm54912wrl1v1gsjr680000gn/T/ipykernel_9725/2762391875.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                           \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                           \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRMSLE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                           \u001b[0mcat_features\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mCAT_COLS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                           \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                           \u001b[0ml2_leaf_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CAT_COLS' is not defined"
     ]
    }
   ],
   "source": [
    "model = CatBoostRegressor(iterations=3000,\n",
    "                          early_stopping_rounds=100,\n",
    "                          grow_policy = 'Depthwise',\n",
    "                          depth=8,\n",
    "                          loss_function=RMSLE(),\n",
    "                          cat_features= CAT_COLS,\n",
    "                          random_state=RS,\n",
    "                          l2_leaf_reg = 1,\n",
    "                          learning_rate=0.03,\n",
    "                          verbose=10,\n",
    "                          eval_metric=RMSLE_val())\n",
    "params = {'l2_leaf_reg':[1,4,8],\n",
    "          'learning_rate': [0.03,0.5,0.1],\n",
    "          'depth':[6,8,10]\n",
    "         }\n",
    "grid_search_res = model.grid_search(params, full_features['items'][FTS_COLS], full_features['items'].target, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52058a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CatBoostRegressor in module catboost.core:\n",
      "\n",
      "class CatBoostRegressor(CatBoost)\n",
      " |  CatBoostRegressor(iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function='RMSE', border_count=None, feature_border_type=None, per_float_feature_quantization=None, input_borders=None, output_borders=None, fold_permutation_block=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, best_model_min_trees=None, verbose=None, silent=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, allow_const_label=None, target_border=None, one_hot_max_size=None, random_strength=None, name=None, ignored_features=None, train_dir=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, pinned_memory_size=None, allow_writing_files=None, final_ctr_computation_mode=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, ctr_description=None, ctr_target_border_count=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, mvs_reg=None, sampling_frequency=None, sampling_unit=None, dev_score_calc_obj_block_size=None, dev_efb_max_buckets=None, sparse_features_conflict_fraction=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, gpu_cat_features_storage=None, data_partition=None, metadata=None, early_stopping_rounds=None, cat_features=None, grow_policy=None, min_data_in_leaf=None, min_child_samples=None, max_leaves=None, num_leaves=None, score_function=None, leaf_estimation_backtracking=None, ctr_history_unit=None, monotone_constraints=None, feature_weights=None, penalties_coefficient=None, first_feature_use_penalties=None, per_object_feature_penalties=None, model_shrink_rate=None, model_shrink_mode=None, langevin=None, diffusion_temperature=None, posterior_sampling=None, boost_from_average=None)\n",
      " |  \n",
      " |  Implementation of the scikit-learn API for CatBoost regression.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  Like in CatBoostClassifier, except loss_function, classes_count, class_names and class_weights\n",
      " |  \n",
      " |  loss_function : string, [default='RMSE']\n",
      " |      'RMSE'\n",
      " |      'MAE'\n",
      " |      'Quantile:alpha=value'\n",
      " |      'LogLinQuantile:alpha=value'\n",
      " |      'Poisson'\n",
      " |      'MAPE'\n",
      " |      'Lq:q=value'\n",
      " |      'SurvivalAft:dist=value;scale=value'\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CatBoostRegressor\n",
      " |      CatBoost\n",
      " |      _CatBoostBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function='RMSE', border_count=None, feature_border_type=None, per_float_feature_quantization=None, input_borders=None, output_borders=None, fold_permutation_block=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, best_model_min_trees=None, verbose=None, silent=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, allow_const_label=None, target_border=None, one_hot_max_size=None, random_strength=None, name=None, ignored_features=None, train_dir=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, pinned_memory_size=None, allow_writing_files=None, final_ctr_computation_mode=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, ctr_description=None, ctr_target_border_count=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, mvs_reg=None, sampling_frequency=None, sampling_unit=None, dev_score_calc_obj_block_size=None, dev_efb_max_buckets=None, sparse_features_conflict_fraction=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, gpu_cat_features_storage=None, data_partition=None, metadata=None, early_stopping_rounds=None, cat_features=None, grow_policy=None, min_data_in_leaf=None, min_child_samples=None, max_leaves=None, num_leaves=None, score_function=None, leaf_estimation_backtracking=None, ctr_history_unit=None, monotone_constraints=None, feature_weights=None, penalties_coefficient=None, first_feature_use_penalties=None, per_object_feature_penalties=None, model_shrink_rate=None, model_shrink_mode=None, langevin=None, diffusion_temperature=None, posterior_sampling=None, boost_from_average=None)\n",
      " |      Initialize the CatBoost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : dict\n",
      " |          Parameters for CatBoost.\n",
      " |          If  None, all params are set to their defaults.\n",
      " |          If  dict, overriding parameters present in dict.\n",
      " |  \n",
      " |  fit(self, X, y=None, cat_features=None, sample_weight=None, baseline=None, use_best_model=None, eval_set=None, verbose=None, logging_level=None, plot=False, column_description=None, verbose_eval=None, metric_period=None, silent=None, early_stopping_rounds=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, init_model=None, callbacks=None, log_cout=<ipykernel.iostream.OutStream object at 0x7fb00331b2b0>, log_cerr=<ipykernel.iostream.OutStream object at 0x7fb00331b370>)\n",
      " |      Fit the CatBoost model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          If not catboost.Pool, 2 dimensional Feature matrix or string - file with dataset.\n",
      " |      \n",
      " |      y : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Labels, 1 dimensional array like.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      cat_features : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving the list of Categ columns indices.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      sample_weight : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Instance weights, 1 dimensional array like.\n",
      " |      \n",
      " |      baseline : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving 2 dimensional array like data.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      use_best_model : bool, optional (default=None)\n",
      " |          Flag to use best model\n",
      " |      \n",
      " |      eval_set : catboost.Pool or list, optional (default=None)\n",
      " |          A list of (X, y) tuple pairs to use as a validation set for\n",
      " |          early-stopping\n",
      " |      \n",
      " |      metric_period : int\n",
      " |          Frequency of evaluating metrics.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If verbose is bool, then if set to True, logging_level is set to Verbose,\n",
      " |          if set to False, logging_level is set to Silent.\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output and\n",
      " |          logging_level is set to Verbose.\n",
      " |      \n",
      " |      silent : bool\n",
      " |          If silent is True, logging_level is set to Silent.\n",
      " |          If silent is False, logging_level is set to Verbose.\n",
      " |      \n",
      " |      logging_level : string, optional (default=None)\n",
      " |          Possible values:\n",
      " |              - 'Silent'\n",
      " |              - 'Verbose'\n",
      " |              - 'Info'\n",
      " |              - 'Debug'\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook\n",
      " |      \n",
      " |      verbose_eval : bool or int\n",
      " |          Synonym for verbose. Only one of these parameters should be set.\n",
      " |      \n",
      " |      early_stopping_rounds : int\n",
      " |          Activates Iter overfitting detector with od_wait set to early_stopping_rounds.\n",
      " |      \n",
      " |      save_snapshot : bool, [default=None]\n",
      " |          Enable progress snapshotting for restoring progress after crashes or interruptions\n",
      " |      \n",
      " |      snapshot_file : string or pathlib.Path, [default=None]\n",
      " |          Learn progress snapshot file path, if None will use default filename\n",
      " |      \n",
      " |      snapshot_interval: int, [default=600]\n",
      " |          Interval between saving snapshots (seconds)\n",
      " |      \n",
      " |      init_model : CatBoost class or string or pathlib.Path, [default=None]\n",
      " |          Continue training starting from the existing model.\n",
      " |          If this parameter is a string or pathlib.Path, load initial model from the path specified by this string.\n",
      " |      \n",
      " |      callbacks : list, optional (default=None)\n",
      " |          List of callback objects that are applied at end of each iteration.\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model : CatBoost\n",
      " |  \n",
      " |  predict(self, data, prediction_type=None, ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type='CPU')\n",
      " |      Predict with data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      prediction_type : string, optional (default='RawFormulaVal')\n",
      " |          Can be:\n",
      " |          - 'RawFormulaVal' : return raw formula value.\n",
      " |          - 'Exponent' : return Exponent of raw formula value.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          If data is for a single object, the return value is single float formula return value\n",
      " |          otherwise one-dimensional numpy.ndarray of formula return values for each object.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Calculate R^2.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          Data to apply model on.\n",
      " |      y : list or numpy.ndarray\n",
      " |          True labels.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      R^2 : float\n",
      " |  \n",
      " |  staged_predict(self, data, prediction_type='RawFormulaVal', ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, verbose=None)\n",
      " |      Predict target at each stage for data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : generator for each iteration that generates:\n",
      " |          If data is for a single object, the return value is single float formula return value\n",
      " |          otherwise one-dimensional numpy.ndarray of formula return values for each object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from CatBoost:\n",
      " |  \n",
      " |  calc_feature_statistics(self, data, target=None, feature=None, prediction_type=None, cat_feature_values=None, plot=True, max_cat_features_on_plot=10, thread_count=-1, plot_file=None)\n",
      " |      Get statistics for the feature using the model, dataset and target.\n",
      " |      To use this function, you should install plotly.\n",
      " |      \n",
      " |      The catboost model has borders for the float features used in it. The borders divide\n",
      " |      feature values into bins, and the model's prediction depends on the number of the bin where the\n",
      " |      feature value falls in.\n",
      " |      \n",
      " |      For float features this function takes model's borders and computes\n",
      " |      1) Mean target value for every bin;\n",
      " |      2) Mean model prediction for every bin;\n",
      " |      3) The number of objects in dataset which fall into each bin;\n",
      " |      4) Predictions on varying feature. For every object, varies the feature value\n",
      " |      so that it falls into bin #0, bin #1, ... and counts model predictions.\n",
      " |      Then counts average prediction for each bin.\n",
      " |      \n",
      " |      For categorical features (only one-hot supported) does the same, but takes feature values\n",
      " |      provided in cat_feature_values instead of borders.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost. Pool or dict {'pool_name': pool} if you want several pools\n",
      " |          Data to compute statistics on\n",
      " |      target: numpy.ndarray or pandas.Series or dict {'pool_name': target} if you want several pools or None\n",
      " |          Target corresponding to data\n",
      " |          Use only if data is not catboost.Pool.\n",
      " |      feature: None, int, string, or list of int or strings\n",
      " |          Features indexes or names in pd.DataFrame for which you want to get statistics.\n",
      " |          None, if you need statistics for all features.\n",
      " |      prediction_type: str\n",
      " |          Prediction type used for counting mean_prediction: 'Class', 'Probability' or 'RawFormulaVal'.\n",
      " |          If not specified, is derived from the model.\n",
      " |      cat_feature_values: list or numpy.ndarray or pandas.Series or\n",
      " |                          dict: int or string to list or numpy.ndarray or pandas.Series\n",
      " |          Contains categorical feature values you need to get statistics on.\n",
      " |          Use dict, when parameter 'feature' is a list to specify cat values for different features.\n",
      " |          When parameter 'feature' is int or str, you can just pass list of cat values.\n",
      " |      plot: bool\n",
      " |          Plot statistics.\n",
      " |      max_cat_features_on_plot: int\n",
      " |          If categorical feature takes more than max_cat_features_on_plot different unique values,\n",
      " |          output result on several plots, not more than max_cat_features_on_plot feature values on each.\n",
      " |          Used only if plot=True or plot_file is not None.\n",
      " |      thread_count: int\n",
      " |          Number of threads to use for getting statistics.\n",
      " |      plot_file: str\n",
      " |          Output file for plot statistics.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict if parameter 'feature' is int or string, else dict of dicts:\n",
      " |          For each unique feature contain\n",
      " |          python dict with binarized feature statistics.\n",
      " |          For float feature, includes\n",
      " |                  'borders' -- borders for the specified feature in model\n",
      " |                  'binarized_feature' -- numbers of bins where feature values fall\n",
      " |                  'mean_target' -- mean value of target over each bin\n",
      " |                  'mean_prediction' -- mean value of model prediction over each bin\n",
      " |                  'objects_per_bin' -- number of objects per bin\n",
      " |                  'predictions_on_varying_feature' -- averaged over dataset predictions for\n",
      " |                  varying feature (see above)\n",
      " |          For one-hot feature, returns the same, but with 'cat_values' instead of 'borders'\n",
      " |  \n",
      " |  calc_leaf_indexes(self, data, ntree_start=0, ntree_end=0, thread_count=-1, verbose=False)\n",
      " |      Returns indexes of leafs to which objects from pool are mapped by model trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Index of first tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Index of the tree after last tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool (default=False)\n",
      " |          Enable debug logging level.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_indexes : 2-dimensional numpy.ndarray of numpy.uint32 with shape (object count, ntree_end - ntree_start).\n",
      " |          i-th row is an array of leaf indexes for i-th object.\n",
      " |  \n",
      " |  compare(self, model, data, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None, log_cout=<ipykernel.iostream.OutStream object at 0x7fb00331b2b0>, log_cerr=<ipykernel.iostream.OutStream object at 0x7fb00331b370>)\n",
      " |      Draw train and eval errors in Jupyter notebook for both models\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model: CatBoost model\n",
      " |          Another model to draw metrics\n",
      " |      \n",
      " |      data : catboost.Pool\n",
      " |          Data to evaluate metrics on.\n",
      " |      \n",
      " |      metrics : list of strings or catboost.metrics.BuiltinMetric\n",
      " |          List of evaluated metrics.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      tmp_dir : string or pathlib.Path (default=None)\n",
      " |          The name of the temporary directory for intermediate results.\n",
      " |          If None, then the name will be generated.\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |  \n",
      " |  create_metric_calcer(self, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None)\n",
      " |      Create batch metric calcer. Could be used to aggregate metric on several pools\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |          Same as in eval_metrics except data\n",
      " |      Returns\n",
      " |      -------\n",
      " |          BatchMetricCalcer object\n",
      " |      \n",
      " |      Usage example\n",
      " |      -------\n",
      " |      # Large dataset is partitioned into parts [part1, part2]\n",
      " |      model.fit(params)\n",
      " |      batch_calcer = model.create_metric_calcer(['Logloss'])\n",
      " |      batch_calcer.add(part1)\n",
      " |      batch_calcer.add(part2)\n",
      " |      metrics = batch_calcer.eval_metrics()\n",
      " |  \n",
      " |  drop_unused_features(self)\n",
      " |      Drop unused features information from model\n",
      " |  \n",
      " |  eval_metrics(self, data, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None, plot=False, log_cout=<ipykernel.iostream.OutStream object at 0x7fb00331b2b0>, log_cerr=<ipykernel.iostream.OutStream object at 0x7fb00331b370>)\n",
      " |      Calculate metrics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool\n",
      " |          Data to evaluate metrics on.\n",
      " |      \n",
      " |      metrics : list of strings or catboost.metrics.BuiltinMetric\n",
      " |          List of evaluated metrics.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      tmp_dir : string or pathlib.Path (default=None)\n",
      " |          The name of the temporary directory for intermediate results.\n",
      " |          If None, then the name will be generated.\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : dict: metric -> array of shape [(ntree_end - ntree_start) / eval_period]\n",
      " |  \n",
      " |  get_all_params(self)\n",
      " |      Get all params (specified by user and default params) that were set in training from CatBoost model.\n",
      " |      Full parameters documentation could be found here: https://catboost.ai/docs/concepts/python-reference_parameters-list.html\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : dict\n",
      " |          Dictionary of {param_key: param_value}.\n",
      " |  \n",
      " |  get_borders(self)\n",
      " |      Return map feature_index: borders for float features.\n",
      " |  \n",
      " |  get_cat_feature_indices(self)\n",
      " |  \n",
      " |  get_embedding_feature_indices(self)\n",
      " |  \n",
      " |  get_feature_importance(self, data=None, type=<EFstrType.FeatureImportance: 2>, prettified=False, thread_count=-1, verbose=False, fstr_type=None, shap_mode='Auto', model_output='Raw', interaction_indices=None, shap_calc_type='Regular', reference_data=None, log_cout=<ipykernel.iostream.OutStream object at 0x7fb00331b2b0>, log_cerr=<ipykernel.iostream.OutStream object at 0x7fb00331b370>)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data :\n",
      " |          Data to get feature importance.\n",
      " |          If type in ('LossFunctionChange', 'ShapValues', 'ShapInteractionValues') data must of Pool type.\n",
      " |              For every object in this dataset feature importances will be calculated.\n",
      " |          If type == 'PredictionValuesChange', data is None or a dataset of Pool type\n",
      " |              Dataset specification is needed only in case if the model does not contain leaf weight information (trained with CatBoost v < 0.9).\n",
      " |          If type == 'PredictionDiff' data must contain a matrix of feature values of shape (2, n_features).\n",
      " |              Possible types are catboost.Pool or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData or pandas.SparseDataFrame or scipy.sparse.spmatrix\n",
      " |          If type == 'FeatureImportance'\n",
      " |              See 'PredictionValuesChange' for non-ranking metrics and 'LossFunctionChange' for ranking metrics.\n",
      " |          If type == 'Interaction'\n",
      " |              This parameter is not used.\n",
      " |      \n",
      " |      type : EFstrType or string (converted to EFstrType), optional\n",
      " |                  (default=EFstrType.FeatureImportance)\n",
      " |          Possible values:\n",
      " |              - PredictionValuesChange\n",
      " |                  Calculate score for every feature.\n",
      " |              - LossFunctionChange\n",
      " |                  Calculate score for every feature by loss.\n",
      " |              - FeatureImportance\n",
      " |                  PredictionValuesChange for non-ranking metrics and LossFunctionChange for ranking metrics\n",
      " |              - ShapValues\n",
      " |                  Calculate SHAP Values for every object.\n",
      " |              - ShapInteractionValues\n",
      " |                  Calculate SHAP Interaction Values between each pair of features for every object\n",
      " |              - Interaction\n",
      " |                  Calculate pairwise score between every feature.\n",
      " |              - PredictionDiff\n",
      " |                  Calculate most important features explaining difference in predictions for a pair of documents.\n",
      " |      \n",
      " |      prettified : bool, optional (default=False)\n",
      " |          used only for PredictionValuesChange type\n",
      " |          change returned data format to the list of (feature_id, importance) pairs sorted by importance\n",
      " |      \n",
      " |      thread_count : int, optional (default=-1)\n",
      " |          Number of threads.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If False, then evaluation is not logged. If True, then each possible iteration is logged.\n",
      " |          If a positive integer, then it stands for the size of batch N. After processing each batch, print progress\n",
      " |          and remaining time.\n",
      " |      \n",
      " |      fstr_type : string, deprecated, use type instead\n",
      " |      \n",
      " |      shap_mode : string, optional (default=\"Auto\")\n",
      " |          used only for ShapValues type\n",
      " |          Possible values:\n",
      " |              - \"Auto\"\n",
      " |                  Use direct SHAP Values calculation only if data size is smaller than average leaves number\n",
      " |                  (the best of two strategies below is chosen).\n",
      " |              - \"UsePreCalc\"\n",
      " |                  Calculate SHAP Values for every leaf in preprocessing. Final complexity is\n",
      " |                  O(NT(D+F))+O(TL^2 D^2) where N is the number of documents(objects), T - number of trees,\n",
      " |                  D - average tree depth, F - average number of features in tree, L - average number of leaves in tree\n",
      " |                  This is much faster (because of a smaller constant) than direct calculation when N >> L\n",
      " |              - \"NoPreCalc\"\n",
      " |                  Use direct SHAP Values calculation calculation with complexity O(NTLD^2). Direct algorithm\n",
      " |                  is faster when N < L (algorithm from https://arxiv.org/abs/1802.03888)\n",
      " |      \n",
      " |      shap_calc_type : EShapCalcType or string, optional (default=\"Regular\")\n",
      " |          used only for ShapValues type\n",
      " |          Possible values:\n",
      " |              - \"Regular\"\n",
      " |                  Calculate regular SHAP values\n",
      " |              - \"Approximate\"\n",
      " |                  Calculate approximate SHAP values\n",
      " |              - \"Exact\"\n",
      " |                  Calculate exact SHAP values\n",
      " |      \n",
      " |      interaction_indices : list of int or string (feature_idx_1, feature_idx_2), optional (default=None)\n",
      " |          used only for ShapInteractionValues type\n",
      " |          Calculate SHAP Interaction Values between pair of features feature_idx_1 and feature_idx_2 for every object\n",
      " |      \n",
      " |      reference_data: catboost.Pool or None\n",
      " |          Reference data for Independent Tree SHAP values from https://arxiv.org/abs/1905.04610v1\n",
      " |          if type == 'ShapValues' and reference_data is not None, then Independent Tree SHAP values are calculated\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      depends on type:\n",
      " |          - FeatureImportance\n",
      " |              See PredictionValuesChange for non-ranking metrics and LossFunctionChange for ranking metrics.\n",
      " |          - PredictionValuesChange, LossFunctionChange, PredictionDiff with prettified=False (default)\n",
      " |              list of length [n_features] with feature_importance values (float) for feature\n",
      " |          - PredictionValuesChange, LossFunctionChange, PredictionDiff with prettified=True\n",
      " |              list of length [n_features] with (feature_id (string), feature_importance (float)) pairs, sorted by feature_importance in descending order\n",
      " |          - ShapValues\n",
      " |              np.ndarray of shape (n_objects, n_features + 1) with Shap values (float) for (object, feature).\n",
      " |              In case of multiclass the returned value is np.ndarray of shape\n",
      " |              (n_objects, classes_count, n_features + 1). For each object it contains Shap values (float).\n",
      " |              Values are calculated for RawFormulaVal predictions.\n",
      " |          - ShapInteractionValues\n",
      " |              np.ndarray of shape (n_objects, n_features + 1, n_features + 1) with Shap interaction values (float) for (object, feature(i), feature(j)).\n",
      " |              In case of multiclass the returned value is np.ndarray of shape\n",
      " |              (n_objects, classes_count, n_features + 1, n_features + 1). For each object it contains Shap interaction values (float).\n",
      " |              Values are calculated for RawFormulaVal predictions.\n",
      " |          - Interaction\n",
      " |              list of length [n_features] of 3-element lists of (first_feature_index, second_feature_index, interaction_score (float))\n",
      " |  \n",
      " |  get_object_importance(self, pool, train_pool, top_size=-1, type='Average', update_method='SinglePoint', importance_values_sign='All', thread_count=-1, verbose=False, ostr_type=None, log_cout=<ipykernel.iostream.OutStream object at 0x7fb00331b2b0>, log_cerr=<ipykernel.iostream.OutStream object at 0x7fb00331b370>)\n",
      " |      This is the implementation of the LeafInfluence algorithm from the following paper:\n",
      " |      https://arxiv.org/pdf/1802.06640.pdf\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pool : Pool\n",
      " |          The pool for which you want to evaluate the object importances.\n",
      " |      \n",
      " |      train_pool : Pool\n",
      " |          The pool on which the model has been trained.\n",
      " |      \n",
      " |      top_size : int (default=-1)\n",
      " |          Method returns the result of the top_size most important train objects.\n",
      " |          If -1, then the top size is not limited.\n",
      " |      \n",
      " |      type : string, optional (default='Average')\n",
      " |          Possible values:\n",
      " |              - Average (Method returns the mean train objects scores for all input objects)\n",
      " |              - PerObject (Method returns the train objects scores for every input object)\n",
      " |      \n",
      " |      importance_values_sign : string, optional (default='All')\n",
      " |          Method returns only Positive, Negative or All values.\n",
      " |          Possible values:\n",
      " |              - Positive\n",
      " |              - Negative\n",
      " |              - All\n",
      " |      \n",
      " |      update_method : string, optional (default='SinglePoint')\n",
      " |          Possible values:\n",
      " |              - SinglePoint\n",
      " |              - TopKLeaves (It is posible to set top size : TopKLeaves:top=2)\n",
      " |              - AllPoints\n",
      " |          Description of the update set methods are given in section 3.1.3 of the paper.\n",
      " |      \n",
      " |      thread_count : int, optional (default=-1)\n",
      " |          Number of threads.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If False, then evaluation is not logged. If True, then each possible iteration is logged.\n",
      " |          If a positive integer, then it stands for the size of batch N. After processing each batch, print progress\n",
      " |          and remaining time.\n",
      " |      \n",
      " |      ostr_type : string, deprecated, use type instead\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object_importances : tuple of two arrays (indices and scores) of shape = [top_size]\n",
      " |  \n",
      " |  get_param(self, key)\n",
      " |      Get param value from CatBoost model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : string\n",
      " |          The key to get param value from.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      value :\n",
      " |          The param value of the key, returns None if param do not exist.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get all params from CatBoost model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : dict\n",
      " |          Dictionary of {param_key: param_value}.\n",
      " |  \n",
      " |  get_text_feature_indices(self)\n",
      " |  \n",
      " |  grid_search(self, param_grid, X, y=None, cv=3, partition_random_seed=0, calc_cv_statistics=True, search_by_train_test_split=True, refit=True, shuffle=True, stratified=None, train_size=0.8, verbose=True, plot=False, log_cout=<ipykernel.iostream.OutStream object at 0x7fb00331b2b0>, log_cerr=<ipykernel.iostream.OutStream object at 0x7fb00331b370>)\n",
      " |      Exhaustive search over specified parameter values for a model.\n",
      " |      Aafter calling this method model is fitted and can be used, if not specified otherwise (refit=False).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      param_grid: dict or list of dictionaries\n",
      " |          Dictionary with parameters names (string) as keys and lists of parameter settings\n",
      " |          to try as values, or a list of such dictionaries, in which case the grids spanned by each\n",
      " |          dictionary in the list are explored.\n",
      " |          This enables searching over any sequence of parameter settings.\n",
      " |      \n",
      " |      X: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |          Data to compute statistics on\n",
      " |      \n",
      " |      y: numpy.ndarray or pandas.Series or None\n",
      " |          Target corresponding to data\n",
      " |          Use only if data is not catboost.Pool.\n",
      " |      \n",
      " |      cv: int, cross-validation generator or an iterable, optional (default=None)\n",
      " |          Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
      " |          - None, to use the default 3-fold cross validation,\n",
      " |          - integer, to specify the number of folds in a (Stratified)KFold\n",
      " |          - one of the scikit-learn splitter classes\n",
      " |              (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\n",
      " |          - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |      \n",
      " |      partition_random_seed: int, optional (default=0)\n",
      " |          Use this as the seed value for random permutation of the data.\n",
      " |          Permutation is performed before splitting the data for cross validation.\n",
      " |          Each seed generates unique data splits.\n",
      " |          Used only when cv is None or int.\n",
      " |      \n",
      " |      search_by_train_test_split: bool, optional (default=True)\n",
      " |          If True, source dataset is splitted into train and test parts, models are trained\n",
      " |          on the train part and parameters are compared by loss function score on the test part.\n",
      " |          After that, if calc_cv_statistics=true, statistics on metrics are calculated\n",
      " |          using cross-validation using best parameters and the model is fitted with these parameters.\n",
      " |      \n",
      " |          If False, every iteration of grid search evaluates results on cross-validation.\n",
      " |          It is recommended to set parameter to True for large datasets, and to False for small datasets.\n",
      " |      \n",
      " |      calc_cv_statistics: bool, optional (default=True)\n",
      " |          The parameter determines whether quality should be estimated.\n",
      " |          using cross-validation with the found best parameters. Used only when search_by_train_test_split=True.\n",
      " |      \n",
      " |      refit: bool (default=True)\n",
      " |          Refit an estimator using the best found parameters on the whole dataset.\n",
      " |      \n",
      " |      shuffle: bool, optional (default=True)\n",
      " |          Shuffle the dataset objects before parameters searching.\n",
      " |      \n",
      " |      stratified: bool, optional (default=None)\n",
      " |          Perform stratified sampling. True for classification and False otherwise.\n",
      " |          Currently supported only for final cross-validation.\n",
      " |      \n",
      " |      train_size: float, optional (default=0.8)\n",
      " |          Should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.\n",
      " |      \n",
      " |      verbose: bool or int, optional (default=True)\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output\n",
      " |          verbose==True is equal to verbose==1\n",
      " |          When verbose==False, there is no messages\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error for every set of parameters in Jupyter notebook\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with two fields:\n",
      " |          'params': dict of best found parameters\n",
      " |          'cv_results': dict or pandas.core.frame.DataFrame with cross-validation results\n",
      " |              columns are: test-error-mean  test-error-std  train-error-mean  train-error-std\n",
      " |  \n",
      " |  iterate_leaf_indexes(self, data, ntree_start=0, ntree_end=0)\n",
      " |      Returns indexes of leafs to which objects from pool are mapped by model trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Index of first tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Index of the tree after last tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_indexes : generator. For each object in pool yields one-dimensional numpy.ndarray of leaf indexes.\n",
      " |  \n",
      " |  load_model(self, fname=None, format='cbm', stream=None, blob=None)\n",
      " |      Load model from a file, stream or blob.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Input file name.\n",
      " |  \n",
      " |  plot_partial_dependence(self, data, features, plot=True, plot_file=None, thread_count=-1)\n",
      " |      To use this function, you should install plotly.\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |      features: int, str, list<int>, tuple<int>, list<string>, tuple<string>\n",
      " |          Float features to calculate partial dependence for. Number of features should be 1 or 2.\n",
      " |      plot: bool\n",
      " |          Plot predictions.\n",
      " |      plot_file: str\n",
      " |          Output file for plot predictions.\n",
      " |      thread_count: int\n",
      " |          Number of threads to use. If -1 use maximum available number of threads.\n",
      " |      Returns\n",
      " |      -------\n",
      " |          If number of features is one - 1d numpy array and figure with line plot.\n",
      " |          If number of features is two - 2d numpy array and figure with 2d heatmap.\n",
      " |  \n",
      " |  plot_predictions(self, data, features_to_change, plot=True, plot_file=None)\n",
      " |      To use this function, you should install plotly.\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |      feature:\n",
      " |          Float features indexes in pd.DataFrame for which you want vary prediction value.\n",
      " |      plot: bool\n",
      " |          Plot predictions.\n",
      " |      plot_file: str\n",
      " |          Output file for plot predictions.\n",
      " |      Returns\n",
      " |      -------\n",
      " |          List of list of predictions for all buckets for all documents in data\n",
      " |  \n",
      " |  plot_tree(self, tree_idx, pool=None)\n",
      " |  \n",
      " |  randomized_search(self, param_distributions, X, y=None, cv=3, n_iter=10, partition_random_seed=0, calc_cv_statistics=True, search_by_train_test_split=True, refit=True, shuffle=True, stratified=None, train_size=0.8, verbose=True, plot=False, log_cout=<ipykernel.iostream.OutStream object at 0x7fb00331b2b0>, log_cerr=<ipykernel.iostream.OutStream object at 0x7fb00331b370>)\n",
      " |      Randomized search on hyper parameters.\n",
      " |      After calling this method model is fitted and can be used, if not specified otherwise (refit=False).\n",
      " |      \n",
      " |      In contrast to grid_search, not all parameter values are tried out,\n",
      " |      but rather a fixed number of parameter settings is sampled from the specified distributions.\n",
      " |      The number of parameter settings that are tried is given by n_iter.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      param_distributions: dict\n",
      " |          Dictionary with parameters names (string) as keys and distributions or lists of parameters to try.\n",
      " |          Distributions must provide a rvs method for sampling (such as those from scipy.stats.distributions).\n",
      " |          If a list is given, it is sampled uniformly.\n",
      " |      \n",
      " |      X: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |          Data to compute statistics on\n",
      " |      \n",
      " |      y: numpy.ndarray or pandas.Series or None\n",
      " |          Target corresponding to data\n",
      " |          Use only if data is not catboost.Pool.\n",
      " |      \n",
      " |      cv: int, cross-validation generator or an iterable, optional (default=None)\n",
      " |          Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
      " |          - None, to use the default 3-fold cross validation,\n",
      " |          - integer, to specify the number of folds in a (Stratified)KFold\n",
      " |          - one of the scikit-learn splitter classes\n",
      " |              (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\n",
      " |          - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |      \n",
      " |      n_iter: int\n",
      " |          Number of parameter settings that are sampled.\n",
      " |          n_iter trades off runtime vs quality of the solution.\n",
      " |      \n",
      " |      partition_random_seed: int, optional (default=0)\n",
      " |          Use this as the seed value for random permutation of the data.\n",
      " |          Permutation is performed before splitting the data for cross validation.\n",
      " |          Each seed generates unique data splits.\n",
      " |          Used only when cv is None or int.\n",
      " |      \n",
      " |      search_by_train_test_split: bool, optional (default=True)\n",
      " |          If True, source dataset is splitted into train and test parts, models are trained\n",
      " |          on the train part and parameters are compared by loss function score on the test part.\n",
      " |          After that, if calc_cv_statistics=true, statistics on metrics are calculated\n",
      " |          using cross-validation using best parameters and the model is fitted with these parameters.\n",
      " |      \n",
      " |          If False, every iteration of grid search evaluates results on cross-validation.\n",
      " |          It is recommended to set parameter to True for large datasets, and to False for small datasets.\n",
      " |      \n",
      " |      calc_cv_statistics: bool, optional (default=True)\n",
      " |          The parameter determines whether quality should be estimated.\n",
      " |          using cross-validation with the found best parameters. Used only when search_by_train_test_split=True.\n",
      " |      \n",
      " |      refit: bool (default=True)\n",
      " |          Refit an estimator using the best found parameters on the whole dataset.\n",
      " |      \n",
      " |      shuffle: bool, optional (default=True)\n",
      " |          Shuffle the dataset objects before parameters searching.\n",
      " |      \n",
      " |      stratified: bool, optional (default=None)\n",
      " |          Perform stratified sampling. True for classification and False otherwise.\n",
      " |          Currently supported only for cross-validation.\n",
      " |      \n",
      " |      train_size: float, optional (default=0.8)\n",
      " |          Should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.\n",
      " |      \n",
      " |      verbose: bool or int, optional (default=True)\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output\n",
      " |          verbose==True is equal to verbose==1\n",
      " |          When verbose==False, there is no messages\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error for every set of parameters in Jupyter notebook\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with two fields:\n",
      " |          'params': dict of best found parameters\n",
      " |          'cv_results': dict or pandas.core.frame.DataFrame with cross-validation results\n",
      " |              columns are: test-error-mean  test-error-std  train-error-mean  train-error-std\n",
      " |  \n",
      " |  save_borders(self, fname)\n",
      " |      Save the model borders to a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or pathlib.Path\n",
      " |          Output file name.\n",
      " |  \n",
      " |  save_model(self, fname, format='cbm', export_parameters=None, pool=None)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Output file name.\n",
      " |      format : string\n",
      " |          Possible values:\n",
      " |              * 'cbm' for catboost binary format,\n",
      " |              * 'coreml' to export into Apple CoreML format\n",
      " |              * 'onnx' to export into ONNX-ML format\n",
      " |              * 'pmml' to export into PMML format\n",
      " |              * 'cpp' to export as C++ code\n",
      " |              * 'python' to export as Python code.\n",
      " |      export_parameters : dict\n",
      " |          Parameters for CoreML export:\n",
      " |              * prediction_type : string - either 'probability' or 'raw'\n",
      " |              * coreml_description : string\n",
      " |              * coreml_model_version : string\n",
      " |              * coreml_model_author : string\n",
      " |              * coreml_model_license: string\n",
      " |          Parameters for PMML export:\n",
      " |              * pmml_copyright : string\n",
      " |              * pmml_description : string\n",
      " |              * pmml_model_version : string\n",
      " |      pool : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series or catboost.FeaturesData\n",
      " |          Training pool.\n",
      " |  \n",
      " |  select_features(self, X, y=None, eval_set=None, features_for_select=None, num_features_to_select=None, algorithm=None, steps=None, shap_calc_type=None, train_final_model=True, verbose=None, logging_level=None, plot=False, log_cout=<ipykernel.iostream.OutStream object at 0x7fb00331b2b0>, log_cerr=<ipykernel.iostream.OutStream object at 0x7fb00331b370>)\n",
      " |      Select best features from pool according to loss value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          If not catboost.Pool, 2 dimensional Feature matrix or string - file with dataset.\n",
      " |      \n",
      " |      y : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Labels, 1 dimensional array like.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      eval_set : catboost.Pool or tuple (X, y) or list [(X, y)], optional (default=None)\n",
      " |          Dataset for evaluation.\n",
      " |      \n",
      " |      features_for_select : str or list of feature indices, names or ranges\n",
      " |          Which features should participate in the selection.\n",
      " |          Format examples:\n",
      " |              - [0, 2, 3, 4, 17]\n",
      " |              - [0, \"2-4\", 17] (both ends in ranges are inclusive)\n",
      " |              - \"0,2-4,20\"\n",
      " |              - [\"Name0\", \"Name2\", \"Name3\", \"Name4\", \"Name20\"]\n",
      " |      \n",
      " |      num_features_to_select : positive int\n",
      " |          How many features to select from features_for_select.\n",
      " |      \n",
      " |      algorithm : EFeaturesSelectionAlgorithm or string, optional (default=RecursiveByShapValues)\n",
      " |          Which algorithm to use for features selection.\n",
      " |          Possible values:\n",
      " |              - RecursiveByPredictionValuesChange\n",
      " |                  Use prediction values change as feature strength, eliminate batch of features at once.\n",
      " |              - RecursiveByLossFunctionChange\n",
      " |                  Use loss function change as feature strength, eliminate batch of features at each step.\n",
      " |              - RecursiveByShapValues\n",
      " |                  Use shap values to estimate loss function change, eliminate features one by one.\n",
      " |      \n",
      " |      steps : positive int, optional (default=1)\n",
      " |          How many steps should be performed. In other words, how many times a full model will be trained.\n",
      " |          More steps give more accurate results.\n",
      " |      \n",
      " |      shap_calc_type : EShapCalcType or string, optional (default=Regular)\n",
      " |          Which method to use for calculation of shap values.\n",
      " |          Possible values:\n",
      " |              - Regular\n",
      " |                  Calculate regular SHAP values\n",
      " |              - Approximate\n",
      " |                  Calculate approximate SHAP values\n",
      " |              - Exact\n",
      " |                  Calculate exact SHAP values\n",
      " |      \n",
      " |      train_final_model : bool, optional (default=True)\n",
      " |          Need to fit model with selected features.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If verbose is bool, then if set to True, logging_level is set to Verbose,\n",
      " |          if set to False, logging_level is set to Silent.\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output and\n",
      " |          logging_level is set to Verbose.\n",
      " |      \n",
      " |      logging_level : string, optional (default=None)\n",
      " |          Possible values:\n",
      " |              - 'Silent'\n",
      " |              - 'Verbose'\n",
      " |              - 'Info'\n",
      " |              - 'Debug'\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook.\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with fields:\n",
      " |          'selected_features': list of selected features indices\n",
      " |          'eliminated_features': list of eliminated features indices\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set parameters into CatBoost model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : key=value format\n",
      " |          List of key=value paris. Example: model.set_params(iterations=500, thread_count=2).\n",
      " |  \n",
      " |  shrink(self, ntree_end, ntree_start=0)\n",
      " |      Shrink the model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ntree_end: int\n",
      " |          Leave the trees with indices from the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Leave the trees with indices from the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |  \n",
      " |  virtual_ensembles_predict(self, data, prediction_type='VirtEnsembles', ntree_end=0, virtual_ensembles_count=10, thread_count=-1, verbose=None)\n",
      " |      Predict with data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      prediction_type : string, optional (default='RawFormulaVal')\n",
      " |          Can be:\n",
      " |          - 'VirtEnsembles': return V (virtual_ensembles_count) predictions.\n",
      " |              k-th virtEnsemle consists of trees [0, T/2] + [T/2 + T/(2V) * k, T/2 + T/(2V) * (k + 1)]  * constant.\n",
      " |          - 'TotalUncertainty': see returned predictions format in 'Returns' part\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      virtual_ensembles_count: int, optional (default=10)\n",
      " |          virtual ensembles count for 'TotalUncertainty' and 'VirtEnsembles' prediction types.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool, optional (default=False)\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          (with V as virtual_ensembles_count and T as trees count,\n",
      " |          k-th virtEnsemle consists of trees [0, T/2] + [T/2 + T/(2V) * k, T/2 + T/(2V) * (k + 1)]  * constant)\n",
      " |          If data is for a single object, return 1-dimensional array of predictions with size depends on prediction type,\n",
      " |          otherwise return 2-dimensional numpy.ndarray with shape (number_of_objects x size depends on prediction type);\n",
      " |          Returned predictions depends on prediction type:\n",
      " |          If loss-function was RMSEWithUncertainty:\n",
      " |              - 'VirtEnsembles': [mean0, var0, mean1, var1, ..., vark-1].\n",
      " |              - 'TotalUncertainty': [mean_predict, KnowledgeUnc, DataUnc].\n",
      " |          otherwise for regression:\n",
      " |              - 'VirtEnsembles':  [mean0, mean1, ...].\n",
      " |              - 'TotalUncertainty': [mean_predicts, KnowledgeUnc].\n",
      " |          otherwise for binary classification:\n",
      " |              - 'VirtEnsembles':  [ApproxRawFormulaVal0, ApproxRawFormulaVal1, ..., ApproxRawFormulaValk-1].\n",
      " |              - 'TotalUncertainty':  [DataUnc, TotalUnc].\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from CatBoost:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _CatBoostBase:\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __deepcopy__(self, _)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  copy(self)\n",
      " |  \n",
      " |  get_best_iteration(self)\n",
      " |  \n",
      " |  get_best_score(self)\n",
      " |  \n",
      " |  get_evals_result(self)\n",
      " |  \n",
      " |  get_leaf_values(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_values : 1d-array of leaf values for all trees.\n",
      " |      Value corresponding to j-th leaf of i-th tree is at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |  \n",
      " |  get_leaf_weights(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_weights : 1d-array of leaf weights for all trees.\n",
      " |      Weight of j-th leaf of i-th tree is at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |  \n",
      " |  get_metadata(self)\n",
      " |  \n",
      " |  get_n_features_in(self)\n",
      " |  \n",
      " |  get_scale_and_bias(self)\n",
      " |  \n",
      " |  get_test_eval(self)\n",
      " |  \n",
      " |  get_test_evals(self)\n",
      " |  \n",
      " |  get_tree_leaf_counts(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      tree_leaf_counts : 1d-array of numpy.uint32 of size tree_count_.\n",
      " |      tree_leaf_counts[i] equals to the number of leafs in i-th tree of the ensemble.\n",
      " |  \n",
      " |  is_fitted(self)\n",
      " |  \n",
      " |  set_feature_names(self, feature_names)\n",
      " |      Sets feature names equal to feature_names\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      feature_names: 1-d array of strings with new feature names in the same order as in pool\n",
      " |  \n",
      " |  set_leaf_values(self, new_leaf_values)\n",
      " |      Sets values at tree leafs of ensemble equal to new_leaf_values.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      new_leaf_values : 1d-array with new leaf values for all trees.\n",
      " |      It's size should be equal to sum(get_tree_leaf_counts()).\n",
      " |      Value corresponding to j-th leaf of i-th tree should be at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |  \n",
      " |  set_scale_and_bias(self, scale, bias)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _CatBoostBase:\n",
      " |  \n",
      " |  best_iteration_\n",
      " |  \n",
      " |  best_score_\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  evals_result_\n",
      " |  \n",
      " |  feature_names_\n",
      " |  \n",
      " |  learning_rate_\n",
      " |  \n",
      " |  n_features_in_\n",
      " |  \n",
      " |  random_seed_\n",
      " |  \n",
      " |  tree_count_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _CatBoostBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _CatBoostBase:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CatBoostRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a23356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
